# Awsome-LM

weekly update latest large model paired with paper(if existed)

#LLM
<div>
<img src="https://img-blog.csdnimg.cn/img_convert/33bda5f420f076384c3013ddd7bd70b6.png">
</div>

- [OpenAI GPT4](https://openai.com/research/gpt-4),the latest milestone in OpenAI’s effort in scaling up deep learning. GPT-4 is a large multimodal model (accepting image and text inputs, emitting text outputs) that, while less capable than humans in many real-world scenarios, exhibits human-level performance on various professional and academic benchmarks.
[paper](https://arxiv.org/abs/2303.08774)
  
- [Alpaca](https://crfm.stanford.edu/alpaca/), Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$). 
[paper](https://arxiv.org/abs/2212.10560)
  
 - [Vicuna](https://vicuna.lmsys.org/), an open-source chatbot trained by fine-tuning LLaMA on user-shared conversations collected from ShareGPT. Preliminary evaluation using GPT-4 as a judge shows Vicuna-13B achieves more than 90%* quality of OpenAI ChatGPT and Google Bard while outperforming other models like LLaMA and Stanford Alpaca in more than 90%* of cases. The cost of training Vicuna-13B is around $300. The code and weights, along with an online demo, are publicly available for non-commercial use.
  
 - [Koala](https://bair.berkeley.edu/blog/2023/04/03/koala/), a chatbot trained by fine-tuning Meta’s LLaMA on dialogue data gathered from the web. We describe the dataset curation and training process of our model, and also present the results of a user study that compares our model to ChatGPT and Stanford’s Alpaca. 

 - [Dolly 2.0](https://www.databricks.com/blog/2023/04/12/dolly-first-open-commercially-viable-instruction-tuned-llm/),  is a 12B parameter language model based on the EleutherAI pythia model family and fine-tuned exclusively on a new, high-quality human generated instruction following dataset, crowdsourced among Databricks employees.

 - [ChatGLM](https://chatglm.cn/blog), a bilingual (English and Chinese) pre-trained language model with 130 billion parameters. It is an attempt to open-source a 100B-scale model at least as good as GPT-3 (davinci) and unveil how models of such a scale can be successfully pre-trained. 
[paper](https://openreview.net/pdf?id=-Aw0rrrPUF)

 - [鹏程.盘古](https://openi.pcl.ac.cn/PCL-Platform.Intelligence/PanGu-Alpha), Alpaca 7B, a model fine-tuned from the LLaMA 7B model on 52K instruction-following demonstrations. On our preliminary evaluation of single-turn instruction following, Alpaca behaves qualitatively similarly to OpenAI’s text-davinci-003, while being surprisingly small and easy/cheap to reproduce (<600$). 
[paper](https://arxiv.org/abs/2212.10560)

- [LaMDA-Language Model for Dialogue Applications](https://blog.google/technology/ai/lamda/), Google's LLM
- [LLaMA-Large Language Model Meta AI](https://ai.facebook.com/blog/large-language-model-llama-meta-ai/), Meta's LLM
- [ERNIE-文心](https://wenxin.baidu.com/), 百度大模型
- [通义千问](https://tongyi.aliyun.com/), 阿里大模型
